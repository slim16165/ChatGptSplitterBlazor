@page "/"
@using Microsoft.AspNetCore.Components
@using OpenAI_API.Models
@using PropertyChanged
@inject ChatGptClient ChatGptClient
@inject IJSRuntime JSRuntime


<PageTitle>ChatGPT Splitter Blazor</PageTitle>

@if (isLoading)
{
    <div class="text-center mt-4">
        <p>Loading...</p>
        <div class="spinner"></div>
    </div>
}



<div class="container my-0" style="padding-bottom: 8rem;">
    <div class="row">
        <div class="col-12">
            <h1><a href="" @onclick="ReloadPage">ChatGPT Splitter</a></h1>

            <p>
                Welcome to <b>ChatGPT Splitter</b>! This tool allows you to...
            </p>
            <hr class="my-3">

            <EditForm Model="formData" OnValidSubmit="HandleValidSubmit">
                @*  <div class="form-group">
                <label for="file">Upload file(s)</label>
                <InputFile id="file" class="form-control-file" @bind-Value="formData.File" multiple />
                </div> *@
                
                @* Token *@
                <div class="form-group mb-4">
                    <label for="apiToken">API Token:</label>
                    <div class="input-group">
                        <InputText id="apiToken" class="form-control" @bind-Value="apiToken" placeholder="Enter your OpenAI API token" />
                        <button @onclick="SaveToken" class="btn btn-primary">Save Token</button>
                    </div>
                </div>

                <div class="form-group">
                    <label for="text">Paste the text to split</label>
                    <InputTextArea id="text" class="form-control" @bind-Value="formData.Text"
                                   rows="8" placeholder="Or paste your text here" />
                </div>


                <div class="row">
                    @* Prompt - Left Column *@
                    <div class="col-8">
                        <label for="prompt" class="pr-2">Prompt:</label>
                        <InputTextArea id="prompt" class="form-control" rows="8" placeholder="Enter prompt here" @bind-Value="formData.Prompt" />
                    </div>

                    @* Token Count, Chunk Size, Number of Chunks - Right Columns *@
                    <div class="col-4 d-flex flex-column justify-content-start mt-4">

                        <div class="d-flex justify-content-between align-items-center mt-2">
                            <label>Token Count:</label>
                            <span>@TokenCount</span>
                        </div>

                        <div class="d-flex justify-content-between align-items-center">

                            <label for="chunk_size" class="pr-2">Chunk size:</label>

                            <InputNumber id="chunk_size" class="form-control" @bind-Value="formData.ChunkSize"
                                         style="width: 70%;" />

                            <button class="btn btn-info ml-2" @onclick="ShowTooltip">
                                <span class="oi oi-info" />
                            </button>

                            @if (showTooltip)
                            {
                                <div class="tooltip">
                                    The OpenAI prompt's maximum capacity is roughly 8192 tokens, equivalent to around 24000 characters for both input and output together.

                                    Dedicating about 2000 characters for the correction prompt leaves approximately 22000 characters available for the input. Ideally, this should be divided evenly between input and output, using 50% for each.

                                    Therefore, the suggested character range for the input field below is between 1,000 and 10,000. However, targeting 3,000 characters is advisable to avoid potential problems.
                                </div>
                            }

                        </div>

                        @code {
        bool showTooltip = false;

        void ShowTooltip()
        {
                            showTooltip = !showTooltip;
        }
                        }

                        <div class="d-flex justify-content-between align-items-center mt-2">
                            <label>Number of Chunks:</label>
                            <InputNumber class="form-control" @bind-Value="NumberOfChunks"
                                         style="width: 70%;" />
                        </div>

                        <div class="d-flex justify-content-between align-items-center mt-2">
                            <label for="model_select" class="pr-2">Model Selection:</label>
                            <select id="model_select" class="form-control" style="width: 70%;" @bind="SelectedModel">
                                <option value="gpt-3.5-turbo">ChatGPTTurbo</option>
                                <option value="gpt-4">GPT-4</option>
                                <option value="gpt-4-32k">GPT4_32k_Context</option>
                            </select>
                        </div>
                    </div>
                </div>

                <div class="form-group mt-4">
                    <InputCheckbox @bind-Value="formData.UseGpt" /> Use GPT for processing
                </div>

                <div class="form-group mt-3">
                    <button type="submit" class="btn btn-primary">Process</button>
                </div>
            </EditForm>
        </div>
    </div>

    <hr class="my-3">
    @foreach (var chunkIndex in Enumerable.Range(0, chunks.Count))
    {
        <ChunkDisplay Chunk="@chunks[chunkIndex]" Response="@(responses.Count > chunkIndex ? responses[chunkIndex] : null)" Index="@chunkIndex" TotalChunks="@chunks.Count" />
    }

</div>

@code {
    protected override void OnInitialized()
    {
        formData = new FormDataViewModel(this);
        NumberOfChunks = 1;
    }


    private string apiToken;

    protected override async Task OnInitializedAsync()
    {
        apiToken = await JSRuntime.InvokeAsync<string>("localStorageManager.getItem", "openai_api_token");
    }

    private async Task SaveToken()
    {
        await JSRuntime.InvokeVoidAsync("localStorageManager.setItem", "openai_api_token", apiToken);
        // Aggiorna il token nel tuo client OpenAI
        ChatGptClient.UpdateToken(apiToken);
    }


    private bool isLoading;
    private FormDataViewModel formData;
    List<string> chunks = new();
    // private readonly ChatGptClient chatGptClient = new ChatGptClient();
    readonly List<string> responses = new();

    public int TokenCount { get; set; }

    public int NumberOfChunks { get; set; }
    public string SelectedModel { get; set; } = "gpt-4";


    public void OnTextChanged()
    {
        using var scope = new LoadingStateScope(this);
        UpdateTextMetrics();
    }

    public void OnChunkSizeChanged()
    {
        UpdateChunkSizeMetrics();
    }

    private void OnNumberOfChunksChanged()
    {
        UpdateChunkSize();
    }

    private static void ReloadPage()
    {
        // Ricarica la pagina
        //NavManager.NavigateTo("/splitter", forceLoad: true);
    }

    private void UpdateTextMetrics()
    {
        if (formData == null)
            return;

        TokenCount = TextProcessingService.CalculateTokenCount(formData.Text);
        NumberOfChunks = TextProcessingService.CalculateNumberOfChunks(TokenCount, formData.ChunkSize);
    }

    private void UpdateChunkSizeMetrics()
    {
        if (formData == null)
            return;

        // Comportamento 2: Ricalcolo in base al cambio di chunkSize
        NumberOfChunks = TextProcessingService.CalculateNumberOfChunks(TokenCount, formData.ChunkSize);
    }

    private void UpdateChunkSize()
    {
        if (formData == null)
            return;

        // Comportamento 3: Ricalcolo in base al cambio di numberOfChunks
        if (NumberOfChunks > 0)
        {
            formData.ChunkSize = (int)Math.Ceiling((double)TokenCount / NumberOfChunks);
        }
    }

    private async Task HandleValidSubmit()
    {
        var textValue = formData.Text;
        var chunkSizeValue = formData.ChunkSize;

        // 1. Conteggio dei Token
        //https://www.nuget.org/packages/Tiktoken/1.1.1#show-readme-container
        var encoding = Tiktoken.Encoding.ForModel("gpt-4");
        var numberOfTokens = encoding.CountTokens(textValue);

        // 2. Determina il Numero di Chunk
        NumberOfChunks = (int)Math.Ceiling((double)numberOfTokens / chunkSizeValue);

        // 3. Trova Punti di Spezzatura Possibili
        List<int> breakPoints = TextProcessingService.FindBreakPoints(textValue);

        // 4. Spezza in Maniera Equilibrata
        chunks = TextProcessingService.SplitTextEqually(textValue, NumberOfChunks, breakPoints);

        // 5. Chiamata OpenAPI per ogni chunk
        responses.Clear(); // Azzera le risposte precedenti
        if (formData.UseGpt)
        {
            foreach (var chunk in chunks)
            {
                await CallOpenApiForChunk(chunk);
            }
        }
    }

    private async Task CallOpenApiForChunk(string chunk)
    {
        using var scope = new LoadingStateScope(this);
        var response = await ChatGptClient.GenerateTextWithForgeAsync(chunk, formData.Prompt, (Model)SelectedModel);
        responses.Add(response);
    }

    [AddINotifyPropertyChangedInterface]
    public class FormDataViewModel
    {
        private readonly ChatGPTSplitter _parent;

        public FormDataViewModel(ChatGPTSplitter parent)
        {
            _parent = parent;
            Text = string.Empty;
            ChunkSize = 15000;
        }

        public string Text { get; set; }
        public string Prompt { get; set; } = "Act like a document/text loader until you load and remember content of the next text/s or document/s.\r\nThere might be multiple files, each file is marked by name in the format ### DOCUMENT NAME.\r\nI will send you them by chunks. Each chunk start will be noted as [START CHUNK x/TOTAL],\r\nand end of this chunk will be noted as [END CHUNK x/TOTAL],\r\nwhere x is number of current chunk and TOTAL is number of all chunks I will send you.\r\nI will send you multiple messages with chunks, for each message just reply OK: [CHUNK x/TOTAL], don't reply anything else, don't explain the text!\r\nLet's begin:";
        public int ChunkSize { get; set; }
        public bool UseGpt { get; set; }

        public void OnTextChanged()
        {
            _parent.UpdateTextMetrics();
        }

        public void OnChunkSizeChanged()
        {
            _parent.UpdateChunkSizeMetrics();
        }
    }

    public class LoadingStateScope : IDisposable
    {
        private readonly ChatGPTSplitter _component;

        public LoadingStateScope(ChatGPTSplitter component)
        {
            _component = component;
            _component.isLoading = true;
            _component.StateHasChanged();
        }

        public void Dispose()
        {
            _component.isLoading = false;
            _component.StateHasChanged();
        }
    }
}

